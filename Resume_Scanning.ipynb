{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divya-105/MajorProjectInterviewChatbot/blob/main/Resume_Scanning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC6whDN4xSJn",
        "outputId": "3576931d-0fc5-42c4-cf2f-2854f230a4a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epMINKQ5lg8d",
        "outputId": "9087fa96-17ea-4670-e019-5a1fc4d2d46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 3.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61102 sha256=c97074b012405683509a0c3d7e279e4007d2d0d84e61f97d2c10517a722ceb40\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=09caae04f961c6c39680ba03b1c935e6fafcd0df6e1862f66dc6928519096dc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install docx2txt\n",
        "#Resume Scanner with Matching Job Description with Given Resume\n",
        "import warnings\n",
        "import PyPDF2 \n",
        "warnings.filterwarnings('ignore')\n",
        "import docx2txt\n",
        "\n",
        "job_description = docx2txt.process('./sample_description.docx')\n",
        "resume = docx2txt.process('./sample_resume3.docx')\n",
        "#print(resume)\n",
        "resume = docx2txt.process('./sample_resume3.docx')\n",
        "\n",
        "# pdfFileObj = open('Resume_M_P.pdf', 'rb') \n",
        "    \n",
        "# # creating a pdf reader object \n",
        "# pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "    \n",
        "# # printing number of pages in pdf file \n",
        "# print(pdfReader.numPages) \n",
        "    \n",
        "# # creating a page object \n",
        "# pageObj = pdfReader.getPage(0) \n",
        "    \n",
        "# # extracting text from page \n",
        "# print(pageObj.extractText()) \n",
        "# resume = pageObj.extractText()\n",
        "# #print(resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADKFa-m3mRNS",
        "outputId": "a0028b44-bd9a-4538-a12f-25b513fcbe52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAJ PANDEY\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Address     : A-305 Surya Kiran CHS Ghansoli Navi Mumbai 400708.\n",
            "\n",
            "Phone        : +91 8850197889\n",
            "\n",
            "Email ID   : raj.pandeyengg@gmail.com\n",
            "\n",
            "Linkedin   : https://bit.ly/3vffztO\n",
            "\n",
            "Github      : https://github.com/raj-1229\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Objective\n",
            "\n",
            "\n",
            "\n",
            "To apply my knowledge and skills to full potential and learn from every opportunity and individual seek the work that increases the performance dimension by constant challenges and window for growth\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Skills\n",
            "\n",
            "\n",
            "\n",
            "Python, C, C++,  C#,  Angular, MongoDB, MySQL \n",
            "\n",
            "ㅡ\n",
            "\n",
            "Experience\n",
            "\n",
            "\n",
            "\n",
            "Codekul Pvt Ltd / Software Development Intern\n",
            "\n",
            "JUNE 2021 - AUGUST 2021 PUNE\n",
            "\n",
            "Completed full stack web development as role of team lead. Developed a full stack application for service management system for different electronics connecting customer and service centers. Technology worked on were ReactJS, MySQL, Springboot\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Projects\n",
            "\n",
            "\n",
            "\n",
            "Interview Bot with Resume Scanning and Face Recognition\n",
            "\n",
            "JAN 2022 - PRESENT\n",
            "\n",
            "The project is based on NLP and Image Processing scanning the resume extracting the key skills of the candidate and taking a photo while submitting the resume and QnA with interview bot based on the extracted skills \n",
            "\n",
            "\n",
            "\n",
            "IOT for Dairy Farm Operations\n",
            "\n",
            "JUNE 2020  - MAY 2021\n",
            "\n",
            "Designed a small working model using arduino  for the idea demonstration of cooling the farm and automatic feeding and watering of the cattles with IOT \n",
            "\n",
            "ㅡ\n",
            "\n",
            "Education\n",
            "\n",
            "\n",
            "\n",
            "MIT Academy of Engineering /BTech Computer Engineering\n",
            "\n",
            "2019 - PRESENT, PUNE\n",
            "\n",
            "Currently in 3rd Year of BTech\n",
            "\n",
            "CGPA : 9.89\n",
            "\n",
            "\n",
            "\n",
            "Shubham Junior College / HSC\n",
            "\n",
            "2016 - 2018 , THANE\n",
            "\n",
            "Maharashtra State Board of Higher Secondary Education\n",
            "\n",
            "Percentage: 91.40%\n",
            "\n",
            "\n",
            "\n",
            "Smt Radhikabai Meghe Vidyalaya / SSC\n",
            "\n",
            "2016\n",
            "\n",
            "Maharashtra State Board of Higher Secondary Education\n",
            "\n",
            "Percentage: 91.38%\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Certifications\n",
            "\n",
            "\n",
            "\n",
            "Python For Everybody (Coursera) \n",
            "\n",
            "Introduction to AI (IUCEE)\n",
            "\n",
            "Introduction to C++\n",
            "\n",
            "MATLAB Onramp \n",
            "\n",
            "Programming Foundations with JavaScript, HTML and CSS\t\n",
            "\n",
            "\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Key Skills \n",
            "\n",
            "\n",
            "\n",
            "Team Leading\n",
            "\n",
            "Decision Making\n",
            "\n",
            "Team Player\n",
            "\n",
            "Time Management\n",
            "\n",
            "Adaptability\n",
            "\n",
            "\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Extra Curricular\n",
            "\n",
            "\n",
            "\n",
            "Formal Anchor for Nakshatra 2022\n",
            "\n",
            "Member of GirlScript MITAOE\n",
            "\n",
            "\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Area of Interest\n",
            "\n",
            "\n",
            "\n",
            " Reading, Traveling \n",
            "\n",
            "\n",
            "\n",
            "ㅡ\n",
            "\n",
            "Language Known\n",
            "\n",
            "\n",
            "\n",
            " English, Hindi, Marathi\n"
          ]
        }
      ],
      "source": [
        "print(job_description)\n",
        "\n",
        "content = [job_description, resume]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_RUlyzQmXl9",
        "outputId": "e504848a-16d8-4666-dc20-aea2518d85d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]]\n",
            "Resume Matches by: 100.00000000000033%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "count_matrix = cv.fit_transform(content)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "mat = cosine_similarity(count_matrix)\n",
        "print(mat)\n",
        "\n",
        "print('Resume Matches by: '+  str(mat[1][0]*100) + '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GDH_TBbtHJe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"PL.csv\")\n",
        "pl = set(df['name'])\n",
        "plList = []\n",
        "for i in pl:\n",
        "    plList.append(i.lower())\n",
        "plList = set(plList)\n",
        "print(plList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQDyWy5Zujmu",
        "outputId": "f80959c6-a533-49c8-e58f-361d07471463"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def skillExtraction(resume):\n",
        "    resume = resume.split()\n",
        "    nr = []\n",
        "    for i in resume:\n",
        "        nr.append(i.lower())\n",
        "    resume = nr\n",
        "    rplList = []\n",
        "    for i in nr:\n",
        "        if(i in plList):\n",
        "            rplList.append(i)\n",
        "    return rplList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhc5-ij1toxn",
        "outputId": "8710b3cd-21b5-4d50-f2e8-40e195b9dfa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['python', 'html']\n",
            "['python', 'html']\n",
            "% age Match with Skills is :  100.0\n"
          ]
        }
      ],
      "source": [
        "jdmList = []\n",
        "jdmList.append(skillExtraction(resume))\n",
        "jdmList.append(skillExtraction(job_description))\n",
        "print(jdmList[0])\n",
        "print(jdmList[1])\n",
        "crs = set(jdmList[0])\n",
        "jds = set(jdmList[1])\n",
        "\n",
        "j = 0\n",
        "for i in crs:\n",
        "    if(i in jds):\n",
        "        j = j+1\n",
        "\n",
        "print(\"% age Match with Skills is : \",(j/len(jdmList[1])*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "wocx_mMX4QOW",
        "outputId": "bfe8f23a-021a-41a7-8340-9e3e08349fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d3da1e0f32bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encapsulationConcept.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Keywords use karke question uthana\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Rather than being dependent on Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'encapsulationConcept.txt'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = open(\"encapsulationConcept.txt\", \"r\")\n",
        "# Keywords use karke question uthana \n",
        "# Rather than being dependent on Dataset\n",
        "# Live Extraction of Data from website then \n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "text = text.readlines()\n",
        "\n",
        "texxt = text\n",
        "text = ''\n",
        "\n",
        "for i in texxt:\n",
        "    text = text + i\n",
        "\n",
        "text =text.split() #splitting the sentence into different word\n",
        "\n",
        "tokens_tag = pos_tag(text) #Getting a list of tuple with word and its POS Tag\n",
        "\n",
        "print(\"After Tokenizing of Words:\",tokens_tag)\n",
        "\n",
        "nounList = [] #To Store different nouns present in the text file\n",
        "\n",
        "#Storing the words in the nounList\n",
        "for i in tokens_tag:\n",
        "    if(i[1]=='NN' or i[1]=='NNP'):\n",
        "        nounList.append(i[0])\n",
        "\n",
        "#Three Types of Question will be formed from Technical Point of View\n",
        "Wq =  'What is ' \n",
        "Eq = 'Explain '\n",
        "Dq = 'Define '\n",
        "\n",
        "#Removing duplicated nouns from the List\n",
        "nounListSet = set(nounList)\n",
        "\n",
        "print(\"\\n**************************************\\n\")\n",
        "#Forming WH Questions\n",
        "for i in nounListSet:\n",
        "    print(Wq+i+\"?\")\n",
        "\n",
        "print(\"\\n**************************************\\n\")\n",
        "#Forming EQ Questions\n",
        "for i in nounListSet:\n",
        "    print(Eq+i+\"?\")\n",
        "\n",
        "\n",
        "print(\"\\n**************************************\\n\")\n",
        "#Forming DQ Questions\n",
        "for i in nounListSet:\n",
        "    print(Dq+i+\"?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqjHuv_yViiC"
      },
      "source": [
        "Stop Words\n",
        "[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLXGWtBeW5Gc"
      },
      "outputs": [],
      "source": [
        "def removeStopWords(x):\n",
        "    stopWords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "    stopWords = set(stopWords)\n",
        "    for i in x:\n",
        "        if(i in stopWords):\n",
        "            x.remove(i)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br2_viz0U7ED",
        "outputId": "14851c7d-0bff-4a11-ec38-4e3f296079cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "x = \"Encapsulation in Java is a mechanism of wrapping the data (variables) and code acting on the data (methods) together as a single unit. In encapsulation, the variables of a class will be hidden from other classes, and can be accessed only through the methods of their current class.\"\n",
        "y = \"will be hidden from other classes, and can be accessed only through the methods of their current class.\"\n",
        "x = x.split()\n",
        "y = y.split()\n",
        "print(len(x))\n",
        "x = removeStopWords(x)\n",
        "y = removeStopWords(y)\n",
        "print(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyU5CCyqYLJq",
        "outputId": "63a20aab-dc49-4989-9955-fb64bcb4120d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encapsulation Java a mechanism wrapping data (variables) code acting data (methods) together a single unit. In encapsulation, variables a class hidden other classes, can be accessed through the methods their current class. \n"
          ]
        }
      ],
      "source": [
        "t = ''\n",
        "for i in x:\n",
        "    t = t+i+' '\n",
        "x = t\n",
        "t = ''\n",
        "for i in y:\n",
        "    t = t+i+' '\n",
        "y = t\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gR-6VkCVv8N"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "content = [x, y]\n",
        "count_matrix = cv.fit_transform(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yi8T4c4cUc_",
        "outputId": "085654bc-eff7-491c-a060-ead7c22fb69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.         0.62764591]\n",
            " [0.62764591 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "mat = cosine_similarity(count_matrix)\n",
        "print(mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltdeA4cocXRe",
        "outputId": "b47f6615-9dbe-4d6c-9e0c-25bc081e6b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer Matches by: 62.764591446084786%\n"
          ]
        }
      ],
      "source": [
        "print('Answer Matches by: '+  str(mat[1][0]*100) + '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w_bvNw4qUacy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"Question_Answer.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bNCs5ZuMNwj7",
        "outputId": "992b8741-5f48-4273-b9fe-26a1e3d77271"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "nounList = [] #To Store different nouns present in the text file\n",
        "\n",
        "for text in df['A']:\n",
        "    j=0\n",
        "    t =text.split() #splitting the sentence into different word\n",
        "    tokens_tag = pos_tag(text) #Getting a list of tuple with word and its POS Tag\n",
        "    for i in tokens_tag:\n",
        "        if(i[1]=='NN' or i[1]=='NNP'):\n",
        "            nounList.append(i[0])\n",
        "            print(nounList)\n",
        "            #df.at[text,j]\n",
        "    j = j+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB-pfc4ZWRMc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sem_6_Major_Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}